{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "## Introduction\n",
    "\n",
    "In this notebook we are going to study gradient descent and in this case applied to logistic regression.\n",
    "Logistic regression (LR) is a statistical method for analysing datasets where there are one of more independent variables that determine the outcome. The outcome is a dichotomous, meaning there are only two possible outcomes (1 / 0, Yes / No, True / False). For instance, if you want to predict the sex of a person from age ($x_1$) and income ($x_2$), the logistic regression model would be\n",
    "\n",
    "$$ h(x) = \\theta_0 + \\theta_1x_1 + \\theta_2 x_2 $$\n",
    "\n",
    "where $h(x)$ is the outcome varibale, $\\theta_0$ the bias and $\\theta_1$ and $\\theta_2$ the weights. The goal is ultimately to tune these parameters with respect to the obeserved data ($x_1$,$x_2$).\n",
    "\n",
    "LR estimates a probability (between 0 and 100%) but $h(x)$ gives values in $(-\\infty, +\\infty)$. We need to \"squish\" $h(x)$ to restrict it to a suitable range. LR commonly uses the logistic function (a.k.a. sigmoid function) to compute probabilities: \n",
    "\n",
    "$$ \\sigma(h(x)) = \\frac{1}{1+e^{-h(x)}}. $$\n",
    "\n",
    "It is possible to threshold the logistic function (values between 0-1), and values below 0.5 will be counted as the prediction of class 0 and values larger than 0.5 results in the prediction of class 1.\n",
    "\n",
    "The full logistic regression model is then:\n",
    "\n",
    "$$ z(x) = \\sigma(h(x)) = \\frac{1}{1+e^{-(\\theta_0 + \\theta_1x_1 + \\theta_2 x_2)}}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ready, steady, code! ðŸš€\n",
    "\n",
    "Let's start with loading some data, scikit-learn comes with a couple of toy datasets and we are going to use the \"iris\" dataset where the goal is to classify which type of flower based on a set of features consisting of sepal length (cm), sepal width (cm), petal length (cm), petal width (cm). To begin with we consider only two of those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T07:50:41.692245Z",
     "start_time": "2021-01-19T07:50:41.685547Z"
    }
   },
   "outputs": [],
   "source": [
    "# import stuff that we need\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets as ds\n",
    "from IPython.display import clear_output # if you get problem with this import you can skip it, it is used to print the cost later which can be skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T07:52:28.766916Z",
     "start_time": "2021-01-19T07:52:28.757691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.2\n",
      "3.5.3\n",
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n",
    "print(mpl.__version__)\n",
    "print(sklearn.__version__)\n",
    "#assert np.__version__ == \"1.19.4\", \"Looks like you don't have the same version of numpy as us!\"\n",
    "#assert mpl.__version__ == \"3.3.3\", \"Looks like you don't have the same version of matplotlib as us!\"\n",
    "#assert sklearn.__version__ == \"0.24.0\", \"Looks like you don't have the same version of sklearn as us!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T07:52:33.179008Z",
     "start_time": "2021-01-19T07:52:33.151459Z"
    }
   },
   "outputs": [],
   "source": [
    "data = ds.load_iris()\n",
    "\n",
    "selected_features_idx = [0,1] #'sepal length (cm)', 'sepal width (cm)'\n",
    "selected_targets = [0,1] #'setosa' 'versicolor'\n",
    "\n",
    "idx = np.array([x in selected_targets for x in data.target])\n",
    "x = data.data[:,selected_features_idx][idx]\n",
    "y = data.target[idx]\n",
    "y[y > 1] = 1 # Reset labels greater than 1 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T07:52:46.508713Z",
     "start_time": "2021-01-19T07:52:46.172908Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "for label in np.unique(y):\n",
    "    plt.scatter(x[:, 0][y == label], x[:, 1][y == label], label = data.target_names[label])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a function that predicts the logistic regression model and make predictions. This function takes a measurement, the current bias and the weights as input.\n",
    "\n",
    "$$ z(x) = \\frac{1}{1+e^{-(\\theta_0 + \\theta_1x_1 + \\theta_2x_2)}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_x(x, bias, weights):\n",
    "    \"\"\" param x: vector containing measurements. x = [x1, x2]\n",
    "        param bias: single value\n",
    "        param weight: vector containing model weights. weights= [w1,w2]\n",
    "        \n",
    "        return: value of logistic regression model for defined x, bias and weights\n",
    "    \"\"\"\n",
    "    return 1/(1 + np.exp(-(bias + weights[0]*x[0] + weights[1]*x[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try it with some random weights and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "bias = np.random.normal()\n",
    "weights = np.random.normal(size = len(x[0]))\n",
    "\n",
    "predicted = []\n",
    "for i in range(len(x)):\n",
    "    yhat = z_x(x[i], bias, weights)\n",
    "    predicted.append(round(yhat))\n",
    "\n",
    "print('Accuracy: ', np.sum(np.equal(y, predicted)) / len(predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the decision boundary between the points for this set of weights. The decision boundary is found by setting $h(x) = 0$ which gives:\n",
    "\n",
    "$$ x_2 = -\\frac{\\theta_0 + \\theta_1x_1}{\\theta_2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = x[:,0]\n",
    "y_values = - (bias + weights[0]*x_values) / weights[1]\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x[:,0][y==0],x[:,1][y==0], label = '0')\n",
    "plt.scatter(x[:,0][y==1],x[:,1][y==1], label = '2')\n",
    "plt.plot(x_values, y_values, label='Decision Boundary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very good (or did you get lucky with the weights?). Try rerunning it a couple of times to see if you can randomly find a better set of weights that improves the accuracy.\n",
    "\n",
    "\n",
    "\n",
    "Now, a better way of finding the optimal weights is the gradient descent method. Gradient descent is an iterative process of minimizing a function by following the gradients of a pre-defined cost function. This is useful for updating and tuning the parameters of our logistic regression model. As gradient descent is an iterative algorithm, we have to repeat the above step until we reach a satisfactory solution. The updates are defined as:\n",
    "\n",
    "$$ \\theta_j \\leftarrow \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial\\theta_j}, $$\n",
    "\n",
    "and similarily for the bias term \n",
    "\n",
    "$$ \\theta_0 \\leftarrow \\theta_0 - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_o}. $$\n",
    "\n",
    "Where $\\alpha$ is a user specified learning rate, a scalar that controls the step size in the parameter space and $J(\\theta)$ is the cost function that we will now define. Note that to minimize the cost function, we move in the direction opposite to the gradient.\n",
    "\n",
    "First, we need to define our cost function, which is typically the negative log-likelihood of the data for numerical reasons, also called *Binary-Cross-Entropy* loss function. For a binary classification problem with $m$ training examples, where $x^{(i)}$ represents the $i$-th example of our training set and $y^{(i)}$ its output, the cost function is given by:\n",
    "\n",
    "$$\n",
    "J(\\theta)=\\frac{1}{m}\\sum_{i=1}^m \n",
    "\\begin{cases}\n",
    "-log(g(x^{(i)}, \\theta)),& \\text{if } y^{(i)} = 1\\\\\n",
    "-log(1 - g(x^{(i)},\\theta)),& \\text{if } y^{(i)} = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $g(x^{(i)},\\theta)$ modelizes $P(y = 1 | x; \\theta)$. Note that we have added $\\theta$ explicitly to the notation to emphasize the dependence on the model parameters.\n",
    "\n",
    "The two functions can be combined into one as:\n",
    "\n",
    "$$ J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m [y^{(i)} \\log(z_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - z_\\theta(x^{(i)}))] $$\n",
    "\n",
    "Where $z_Î¸(x)$ is the sigmoid function, representing the probability that the input $x$ belongs to the positive class.\n",
    "\n",
    "To simplify the notation in the following calculations, we will omit the subindexes; nevertheless, keep in mind that when the cost function is optimized, it is done across all samples in the training dataset.\n",
    "\n",
    "$$ J(\\theta)= -y\\cdot log(z(x)) - (1-y)\\cdot log(1-z(x))$$\n",
    "\n",
    "where $y$ is the target class. The Binary-Cross-Entropy tells us that if the target is 1 and we predict 0, then we will get a large error ($-log(0) = \\infty$) and vice verca ($-log(1 - 1) = -log(0) = \\infty$).\n",
    "\n",
    "For gradient descent we need the derivative of this cost function with respect to the weights $\\frac{\\partial J(\\theta)}{\\partial\\theta_j}$. We can get this with the chain rule:\n",
    "\n",
    "\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{\\partial J(\\theta)}{\\partial z(x)} \\cdot \\frac{\\partial z(x)}{\\partial h(x)} \\cdot \\frac{\\partial h(x)}{\\partial \\theta_j}$$\n",
    "\n",
    "\n",
    "Where the three derivatives result in:\n",
    "\n",
    "$$\\begin{aligned} \\frac{\\partial J(\\theta)}{\\partial z(x)} &= -\\left(\\frac{y}{z(x)} - \\frac{(1-y)}{(1-z(x))}\\right) \\\\ \\frac{\\partial z(x)}{\\partial h(x)} &= z(x)\\cdot(1-z(x)) \\\\ \\frac{\\partial h(x)}{\\partial \\theta} &=  x \\end{aligned}$$\n",
    "\n",
    "Combining the previous equations together with the previous chain rule gives \n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial\\theta_j} = x_j\\cdot(z(x)-y) $$\n",
    "\n",
    "where $x_j$ is the $j$-th component of $x$.\n",
    "\n",
    "For the bias term the derivative is similar but it is not dependent on $x$ since $\\frac{\\partial h(x)}{\\partial \\theta_0} = Â 1$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = z(x)-y $$\n",
    "\n",
    "Finally, recovering the subindex notation for all the samples in our training set we can express the gradient of the cost function with respect to each parameter. For the weight case:\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m (z_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}. $$\n",
    "\n",
    "And for the bias case:\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m (z_\\theta(x^{(i)}) - y^{(i)}). $$\n",
    "\n",
    "With this two formulas now we can use these gradients to update the parameters in the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The full algorithm is:**\n",
    "1. Initialize the weights randomly. \n",
    "2. Calculate the gradients of cost function w.r.t parameters.\n",
    "3. Update the weights by $ \\theta_j \\leftarrow \\theta_j - \\alpha \\frac{\\partial}{\\partial\\theta_j}J(\\theta). $\n",
    "4. Update the bias by $ \\theta_0 \\leftarrow \\theta_0 - \\alpha \\frac{\\partial}{\\partial \\theta_0}J(\\theta). $\n",
    "5. Repeat until value of cost function does not change or to a pre-defined number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function for the cost and one for its derivative with respect to the weights and one with respect to the bias. Note that the derivative function will return the number of values corresponing the the number of weights that you have. Also note that we are only doing this for one training point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(y, x, bias, weights):\n",
    "    \"\"\" param y: Ground truth label for measurements\n",
    "        param x: vector containing measurements. x =Â [x1, x2]\n",
    "        param bias: single value\n",
    "        param weight: vector containing model weights. weights= [w1,w2]\n",
    "    \n",
    "        return: value of the cost function. In this case BCE\n",
    "    \"\"\"\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_weights(y, x, bias, weights):\n",
    "    \"\"\" param y: Ground truth label for measurements\n",
    "        param x: vector containing measurements. x =Â [x1, x2]\n",
    "        param bias: single value\n",
    "        param weight: vector containing model weights. weights= [w1,w2]\n",
    "    \n",
    "        return: derivative of cost function with respect to the weights, dw = [dw1, dw2]\n",
    "    \"\"\"\n",
    "    return cost_deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_bias(y, x, bias, weights):\n",
    "    \"\"\" param y: Ground truth label for measurements\n",
    "        param x: vector containing measurements. x =Â [x1, x2]\n",
    "        param bias: single value\n",
    "        param weight: vector containing model weights. weights= [w1,w2]\n",
    "    \n",
    "        return: derivative of cost function with respect to the bias\n",
    "    \"\"\"\n",
    "    return cost_deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally lets fit the logistic regression model with gradient descent across all training data points. As we saw before, gradient descent works by, at each iteration, average the total cost and the derivatives on over the full training set.\n",
    "\n",
    "Implement gradient descent for logistic regression. Experiment with different learning rates and number of iterations to see if you get differnt solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = ... # <-- specify learning rate\n",
    "\n",
    "# Initialize weights and bias as random\n",
    "bias = np.random.normal()\n",
    "weights = np.random.normal(size = len(x[0]))\n",
    "\n",
    "\n",
    "number_of_iterations = ... # <-- number of iterations to perform gradient descent\n",
    "\n",
    "# Loop through training data and update the weights at each iteration\n",
    "\n",
    "for it in range(number_of_iterations):\n",
    "# .. Code for gradient descent for logistic regression\n",
    "    \n",
    "    \n",
    "    clear_output(wait=True) # This is used to clear the output for cleaner printing, can be removed if it causes trouble.\n",
    "    print('iteration: ', it, ' cost: ', cost) # In this case the variable for the current cost is called \"cost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []\n",
    "for i in range(len(x)):\n",
    "    yhat = z_x(x[i], bias, weights)\n",
    "    predicted.append(round(yhat))\n",
    "\n",
    "print('Accuracy: ', np.sum(np.equal(y,predicted)) / len(predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the decision boundary for the new weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = x[:,0]\n",
    "y_values = - (bias + weights[0]*x_values) / weights[1]\n",
    "\n",
    "plt.figure()\n",
    "for label in np.unique(y):\n",
    "    plt.scatter(x[:,0][y==label],x[:,1][y==label], label = data.target_names[label])\n",
    "plt.plot(x_values, y_values, label='Decision Boundary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
